{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: Improve model with a better optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'https://n1c1w90i0f.clg07azjl.paperspacegradient.com/'. Verify the server is running and reachable. (Failed to connect to the remote Jupyter Server 'https://n1c1w90i0f.clg07azjl.paperspacegradient.com/'. Verify the server is running and reachable. (Invalid response: 503 Service Unavailable).)."
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, clone_model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import L1L2\n",
    "\n",
    "import helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "x_train, y_train, x_test, y_test = helpers.load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recompile the same model with Adam, which is ususally prefered over SGD for image classification. Reasons:\n",
    "\n",
    "- Adaptive Learning Rate: Adam adapts the learning rate for each parameter, making it more efficient in handling sparse gradients or noisy data, while SGD uses a fixed learning rate unless momentum or schedules are applied.\n",
    "\n",
    "- Faster Convergence: Adam combines momentum and adaptive learning rates, helping it converge faster and more efficiently, especially in complex models.\n",
    "\n",
    "- Less Hyperparameter Tuning: Adam works well with default settings, whereas SGD typically requires more careful tuning of learning rates and momentum.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape=input_shape))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# implement callbacks\n",
    "lr_schedule = LearningRateScheduler(lambda epoch: 1e-3 * 10**(-epoch / 20))\n",
    "\n",
    "# train model\n",
    "history = model.fit(\n",
    "    x_train, y_train, \n",
    "    batch_size=512, \n",
    "    epochs=15, \n",
    "    validation_split=0.1,\n",
    "    callbacks=[lr_schedule]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "helpers.evaluate_model(model, x_test, y_test)\n",
    "helpers.plot_model_history(history)\n",
    "helpers.plot_confusion_matrix(model, x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
